{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a57171-cf4a-46c9-bfc3-221659628e8c",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Ans: Min-maxing is a statistical technique for re-scaling numerical values into a [0,1] range.\n",
    "\n",
    "Transform features by scaling each feature to a given range.\n",
    "\n",
    "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "\n",
    "The transformation is given by:\n",
    "\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "X_scaled = X_std * (max - min) + min\n",
    "\n",
    "where min, max = feature_range.\n",
    "\n",
    "This transformation is often used as an alternative to zero mean, unit variance scaling.\n",
    "\n",
    "For example, a series of album ratings scaled from 70 to 150 could be min-maxed so that every rating falls on or between 0 and 1, and the proportional distance between data points is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a5119-c5c7-4c27-8be9-776701e5746d",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Ans: the Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite useful.\n",
    "Scaling is done considering the whole feature vector to be of unit length.\n",
    "\n",
    " min-max normalization, where we scale the values to a range of 0 to 1, or unit vector normalization, where we scale the values to have a length of 1.\n",
    "\n",
    "For example, when dealing with image data, the colors can range from only 0 to 255. If we plot, then it would look as below for L1 and L2 norm, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8aff9-ceff-4551-9215-6f048dc1f132",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Ans: Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset while preserving maximum variation. It transforms the original variables into a new set of linearly uncorrelated variables called principal components.\n",
    "It is a projection based method that transforms the data by projecting it onto a set of orthogonal(perpendicular) axes\n",
    "\n",
    "PCA can be used to reduce the size of an image without significantly impacting the quality. Beyond just reducing the size, this is useful for image classification algorithms. Visualizing multidimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12562354-730d-4392-b327-a54dea45813c",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Ans: Feature extraction: PCA can be used to identify the most important features in a dataset, which can be used to build predictive models. Visualization: PCA can be used to visualize high-dimensional data in two or three dimensions, making it easier to understand and interpret.\n",
    "\n",
    "PCA\n",
    "PCA is a dimensionality reduction that identifies important relationships in our data, transforms the existing data based on these relationships, and then quantifies the importance of these relationships so we can keep the most important relationships and drop the others. \n",
    "\n",
    "1) We identify the relationship among features through a Covariance Matrix.\n",
    "2) Through the linear transformation or eigendecomposition of the Covariance Matrix, we get eigenvectors and eigenvalues.\n",
    "3) Then we transform our data using Eigenvectors into principal components.\n",
    "4) Lastly, we quantify the importance of these relationships using Eigenvalues and keep the important principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d7d27-d205-432d-b5e7-61e024e01fc7",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Ans: Min-Max scaling is a common preprocessing technique used in machine learning to scale and normalize features within a specific range, typically between 0 and 1. This is particularly useful when dealing with features that have different units or ranges, as it ensures that all features contribute equally to the model's training process. In the context of building a recommendation system for a food delivery service with features like price, rating, and delivery time, you can follow these steps to use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1) Understand the Data:\n",
    "\n",
    "Begin by thoroughly understanding your dataset, including the distribution of the features and their possible ranges. In your case, you have features like price, rating, and delivery time.\n",
    "\n",
    "2) Choose the Scaling Range:\n",
    "\n",
    "Determine the range within which you want to scale your features. The common choice is [0, 1], but you can choose a different range based on your specific needs.\n",
    "\n",
    "3) Calculate the Minimum and Maximum Values:\n",
    "\n",
    "For each feature (price, rating, delivery time), calculate the minimum (min) and maximum (max) values across the entire dataset.\n",
    "\n",
    "4) Apply Min-Max Scaling:\n",
    "\n",
    "For each data point in your dataset, apply the Min-Max scaling formula to scale each\n",
    "\n",
    "scaled_value = (x - min) / (max - min)\n",
    "\n",
    "Where:\n",
    "\n",
    "x is the original value of the feature.\n",
    "\n",
    "min is the minimum value of the feature in the dataset.\n",
    "\n",
    "max is the maximum value of the feature in the dataset.\n",
    "\n",
    "This formula scales each data point in the feature to a value between 0 and 1. Data points closer to 0 indicate a lower value relative to the minimum, while data points closer to 1 indicate a higher value relative to the maximum.\n",
    "\n",
    "5) Repeat for Each Feature: Apply the Min-Max scaling transformation independently to each feature you want to scale.\n",
    "\n",
    "6) Replace Original Data: After scaling all relevant features, you can replace the original values in your dataset with their scaled counterparts. Make sure to keep the original values if you need them for any other purposes.\n",
    "\n",
    "7) Use the Scaled Data: The scaled data can now be used as input for your recommendation system algorithm. Since all the features are on the same scale, they will have an equal influence on the model's predictions, preventing any feature from dominating due to its scale.\n",
    "\n",
    "Min-Max scaling helps normalize your data and can improve the performance and convergence of machine learning algorithms, especially those sensitive to feature scales, like distance-based algorithms (e.g., k-nearest neighbors) and gradient descent-based algorithms (e.g., neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f44f2-c741-478e-96e7-d5c89c48de48",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Ans: Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis to simplify high-dimensional datasets while retaining most of the relevant information. In the context of building a model to predict stock prices using a dataset with numerous features, including company financial data and market trends, here's how you can use PCA to reduce dimensionality:\n",
    "\n",
    "Step 1: Data Preprocessing\n",
    "\n",
    "Before applying PCA, it's essential to preprocess your data:\n",
    "\n",
    "Handling Missing Data: Address any missing data by either removing rows with missing values, imputing missing values, or using more advanced imputation techniques.\n",
    "\n",
    "Feature Scaling: Standardize or normalize your features to ensure they all have the same scale. PCA is sensitive to the scales of the features, so this step is crucial.\n",
    "\n",
    "Step 2: Calculate the Covariance Matrix\n",
    "\n",
    "PCA works by finding linear combinations of your original features, which capture the most significant variance in the data. To do this, you need to compute the covariance matrix of your standardized or normalized dataset. The covariance matrix measures the relationships between pairs of features and is essential for finding the principal components.\n",
    "\n",
    "Step 3: Eigendecomposition of the Covariance Matrix\n",
    "\n",
    "After obtaining the covariance matrix, you perform an eigendecomposition to find the eigenvectors and eigenvalues. The eigenvectors represent the principal components of the data, and the eigenvalues indicate the amount of variance each principal component explains.\n",
    "\n",
    "Step 4: Select Principal Components\n",
    "\n",
    "To reduce the dimensionality of your dataset, you'll need to decide how many principal components to keep. A common approach is to sort the eigenvalues in descending order and choose the top k eigenvectors that explain most of the variance in the data. You can choose k based on a desired level of explained variance (e.g., 95% of the total variance).\n",
    "\n",
    "Step 5: Transform the Data\n",
    "\n",
    "Once you've selected the principal components, you can transform your original dataset into a lower-dimensional space by projecting it onto the subspace defined by these components. This is done by multiplying your data by the matrix of selected eigenvectors.\n",
    "\n",
    "Step 6: Use the Reduced-Dimensional Data\n",
    "\n",
    "The reduced-dimensional dataset, obtained after PCA, can now be used as input for your stock price prediction model. This dataset contains fewer features (equal to the number of selected principal components) while retaining most of the relevant information from the original data. You can apply various machine learning algorithms to this reduced dataset, such as linear regression, support vector machines, or neural networks.\n",
    "\n",
    "Benefits of using PCA for dimensionality reduction in stock price prediction:\n",
    "\n",
    "Curse of Dimensionality: High-dimensional datasets can suffer from the curse of dimensionality, making modeling and analysis challenging. PCA helps mitigate this issue by reducing the number of features.\n",
    "\n",
    "Noise Reduction: PCA can also help in reducing noise and focusing on the most significant patterns and relationships in the data.\n",
    "\n",
    "Improved Model Performance: By reducing dimensionality while preserving relevant information, PCA can often lead to more efficient models that generalize better to new data.\n",
    "\n",
    "Interpretability: Fewer dimensions are easier to interpret and visualize, making it easier to understand the driving factors behind stock price movements.\n",
    "\n",
    "However, it's essential to keep in mind that while PCA is a powerful technique, it may not always lead to better results. The choice of the number of principal components (k) should be made carefully, and the impact on model performance should be evaluated through cross-validation or other validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d39a92-b2b6-4e64-aaba-18e932420eac",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "Ans: Formula:\n",
    "    \n",
    "    scaled_value = (x - min) / (max - min) * (new_max - new_min) + new_min\n",
    "    x is the original value.\n",
    "    min is the minimum value in your dataset (1 in this case).\n",
    "    max is the maximum value in your dataset (20 in this case).\n",
    "    new_min is the minimum value of the new range (-1 in this case).\n",
    "    new_max is the maximum value of the new range (1 in this case).\n",
    "    \n",
    "For the value 1:\n",
    "scaled_value = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0\n",
    "\n",
    "For the value 5:\n",
    "scaled_value = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -0.6\n",
    "\n",
    "For the value 10:\n",
    "scaled_value = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -0.2\n",
    "\n",
    "For the value 15:\n",
    "scaled_value = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.2\n",
    "\n",
    "For the value 20:\n",
    "scaled_value = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.6\n",
    "\n",
    "So, the Min-Max scaled values for your dataset in the range of -1 to 1 are:\n",
    "[0, -0.6, -0.2, 0.2, 0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448b279-53e7-4a8a-9ee1-66993c8c843e",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Ans: Performing feature extraction using PCA involves reducing the dimensionality of your dataset by projecting it onto a lower-dimensional subspace while retaining as much variance as possible. In your case, you have a dataset with features: height, weight, age, gender, and blood pressure. To decide how many principal components to retain, you typically consider the amount of variance explained by each component and choose a number that retains enough information for your specific use case. Here's a general approach:\n",
    "\n",
    "Standardize or Normalize Data: Before applying PCA, it's essential to standardize or normalize your data, as PCA is sensitive to feature scales.\n",
    "\n",
    "Calculate Covariance Matrix: Compute the covariance matrix of your standardized or normalized data. This matrix will be used to find the principal components.\n",
    "\n",
    "Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "Explained Variance: Calculate the explained variance for each principal component. The explained variance of a component is the ratio of its eigenvalue to the sum of all eigenvalues. It represents the proportion of the total variance in the data explained by that component.\n",
    "\n",
    "Cumulative Explained Variance: Calculate the cumulative explained variance as you consider more principal components. This cumulative value helps you understand how much of the total variance is retained as you add components.\n",
    "\n",
    "Choose the Number of Components: Decide how many principal components to retain based on your desired level of explained variance. A common threshold is to retain enough components to explain a certain percentage of the total variance (e.g., 95% or 99%). You can also use a scree plot or elbow method to visualize the point at which adding more components doesn't significantly increase explained variance.\n",
    "\n",
    "The choice of how many principal components to retain depends on your specific objectives and trade-offs between dimensionality reduction and information retention. Here are some considerations:\n",
    "\n",
    "If you aim to reduce dimensionality while retaining most of the information, you might choose a threshold like 95% of the total variance. This ensures that you retain the majority of the variance in the original data.\n",
    "\n",
    "If interpretability is crucial, you might choose a smaller number of components that still capture a reasonable amount of variance. Fewer components are easier to interpret and visualize.\n",
    "\n",
    "If you plan to use the reduced dataset for machine learning, you can perform cross-validation with different numbers of components to assess their impact on model performance.\n",
    "\n",
    "Ultimately, the choice of the number of principal components should align with the goals of your analysis or modeling task. There is no one-size-fits-all answer, and the decision often involves a balance between dimensionality reduction and information preservation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
